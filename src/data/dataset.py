from typing import List, Optional, Union, Dict
import logging

from torch.utils.data import Dataset
from torch import cuda
import pandas as pd

from src.utils.commons import RegisterableObject
from src.preprocessing.base import BasePreprocessing
from src.data.commons import Message, Conversation

logger = logging.getLogger()


"""
expected structure of `raw_data_path`:
    /path/to/raw/data/
        - train.csv
        - test.csv
        - train-m2m100-1.2B-f16-fr.csv
        - train-nllb-3.3B-de.csv
        - processed: # auto-generated directory

"""

class BaseDataset(Dataset, RegisterableObject):
    """
    args:
        raw_data_path: str          -> path to a directory that contains files representing the raw data of this dataset.
                                       These files should be in format of csv and has the following columns (todo)
        rerun_prepartion: bool     -> rerun the pipeline from beginning and do not retrieve it from files
        apply_record_filter: bool   -> should apply filters on each record
        preprocessings              -> a list of preprocessings that are applied on each sentence in order
        device                      -> the device that vectorization works on. It is NOT supposed to move tensors to cuda during training (auto -> if possible cuda else cpu)
    """
    # do not implement preprocessings for now
    def __init__(self, raw_data_path: str, Dict, rerun_prepartion: bool = False, embedding_path: Optional[str] = '', tokenizer_path: Optional[str] = '', apply_record_filter: bool = True,
                 preprocessings: Optional[List[BasePreprocessing]] = None, device: Union[str, cuda.device]='str', aggregate: bool = True) -> None:
        super().__init__()
        self.raw_data_path = raw_data_path
        self.rerun_prepartion = rerun_prepartion
        self.apply_record_filter = apply_record_filter
        self.preprocessings = preprocessings
        self.device = device
        self.aggregate = aggregate
        
        self.embedding_path = embedding_path
        self.tokenizer_path = tokenizer_path
        self.vector_based = True # considering the default that embedding is only passed
        
        if self.tokenizer_path is not None:
            self.vector_based = False

        # checks
        if self.embedding_path is not None and self.tokenizer_path is not None:
            raise ValueError("only one of `embedding_path` or `tokenizer_path` should be defined")
        
        if self.embedding_path is None and self.tokenizer_path is None:
            raise ValueError("at least one of `embedding_path` or `tokenizer_path` should be defined")
    
    def __getitem__(self, index):
        if self.vector_based:
            return # todo
    
    # prepare according to requirements of pipeline
    def prepare(self):
        pass


dataset = {
    "disilroberta-bt-nllb-fr-message-sequence": {
        'raw_data_path': 'data/dataset-v3/pan12/',
        'language': 'en', # default for pan12 dataset
        'backtranslation': [('nllb', 'fr')], # list of backtranslation pairs that are going to be appended to the end of file
        'embedding_path':'distilroberta', # `embedding_path` and `tokenizer_path` are exclusive with each other
        'tokenizer_path': None,  # if it is passed, the dataset only returns the tokens ids and mask etc generated by the tokenizer
        'preprocessings': [],
        'apply_record_filter': True,
        'device': 'auto',
        'aggregate': False,
    }
}
